{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4226061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukino\\AppData\\Local\\Temp/ipykernel_34184/478973545.py:118: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ], dtype=np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  3. nan  1.]\n",
      " [ 4. nan nan  1.]\n",
      " [ 1.  1. nan  5.]\n",
      " [ 1. nan nan  4.]\n",
      " [nan  1.  5.  4.]]\n",
      "Iteration: 10 ; error = 0.0343\n",
      "Iteration: 20 ; error = 0.0386\n",
      "Iteration: 30 ; error = 0.0228\n",
      "Iteration: 40 ; error = 0.0305\n",
      "Iteration: 50 ; error = 0.0262\n",
      "Iteration: 60 ; error = 0.0396\n",
      "Iteration: 70 ; error = 0.0349\n",
      "Iteration: 80 ; error = 0.0251\n",
      "Iteration: 90 ; error = 0.0387\n",
      "Iteration: 100 ; error = 0.0364\n",
      "[[4.90058801 2.98388394 3.51046716 1.00968118]\n",
      " [3.97344107 2.2388166  3.30461546 1.0454593 ]\n",
      " [1.03379772 1.00710775 4.71911839 4.90353778]\n",
      " [1.04454903 0.82469777 4.02550448 3.94912236]\n",
      " [1.82940926 1.04890343 4.94209843 3.94897586]]\n",
      "[[5.         3.         3.51046716 1.        ]\n",
      " [4.         2.2388166  3.30461546 1.        ]\n",
      " [1.         1.         4.71911839 5.        ]\n",
      " [1.         0.82469777 4.02550448 4.        ]\n",
      " [1.82940926 1.         5.         4.        ]]\n",
      "[[ 5.  3. nan  1.]\n",
      " [ 4. nan nan  1.]\n",
      " [ 1.  1. nan  5.]\n",
      " [ 1. nan nan  4.]\n",
      " [nan  1.  5.  4.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "class MF():\n",
    " \n",
    "    def __init__(self, X, k, alpha, beta, iterations):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict np.nan entries in a matrix.\n",
    "        Arguments\n",
    "        - X (ndarray)   : sample-feature matrix\n",
    "        - k (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.num_samples, self.num_features = X.shape\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        # True if not nan\n",
    "        self.not_nan_index = (np.isnan(self.X) == False)\n",
    " \n",
    "    def train(self):\n",
    "        # Initialize factorization matrix U and V\n",
    "        self.U = np.random.normal(scale=1./self.k, size=(self.num_samples, self.k))\n",
    "        self.V = np.random.normal(scale=1./self.k, size=(self.num_features, self.k))\n",
    " \n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_samples)\n",
    "        self.b_v = np.zeros(self.num_features)\n",
    "        self.b = np.mean(self.X[np.where(self.not_nan_index)])\n",
    "        # Create a list of training samples, 将矩阵不为0的每个元素作为样本\n",
    "        self.samples = [\n",
    "            (i, j, self.X[i, j])\n",
    "            for i in range(self.num_samples)\n",
    "            for j in range(self.num_features)\n",
    "            if not np.isnan(self.X[i, j])\n",
    "        ]\n",
    " \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            # total square error\n",
    "            se = self.square_error()\n",
    "            training_process.append((i, se))\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, se))\n",
    " \n",
    "        return training_process\n",
    " \n",
    "    def square_error(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total square error\n",
    "        \"\"\"\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for i in range(self.num_samples):\n",
    "            for j in range(self.num_features):\n",
    "                if self.not_nan_index[i, j]:\n",
    "                    error += pow(self.X[i, j] - predicted[i, j], 2)\n",
    "        return error\n",
    " \n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, x in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_x(i, j)\n",
    "            e = (x - prediction)\n",
    " \n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (2 * e - self.beta * self.b_u[i])\n",
    "            self.b_v[j] += self.alpha * (2 * e - self.beta * self.b_v[j])\n",
    " \n",
    "            # Update factorization matrix U and V\n",
    "            \"\"\"\n",
    "            If RuntimeWarning: overflow encountered in multiply,\n",
    "            then turn down the learning rate alpha.\n",
    "            \"\"\"\n",
    "            self.U[i, :] += self.alpha * (2 * e * self.V[j, :] - self.beta * self.U[i,:])\n",
    "            self.V[j, :] += self.alpha * (2 * e * self.U[i, :] - self.beta * self.V[j,:])\n",
    " \n",
    "    def get_x(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted x of sample i and feature j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_v[j] + self.U[i, :].dot(self.V[j, :].T)\n",
    "        return prediction\n",
    " \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, U and V\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:, np.newaxis] + self.b_v[np.newaxis, :] + self.U.dot(self.V.T)\n",
    " \n",
    "    def replace_nan(self, X_hat):\n",
    "        \"\"\"\n",
    "        Replace np.nan of X with the corresponding value of X_hat\n",
    "        \"\"\"\n",
    "        X = np.copy(self.X)\n",
    "        for i in range(self.num_samples):\n",
    "            for j in range(self.num_features):\n",
    "                if np.isnan(X[i, j]):\n",
    "                    X[i, j] = X_hat[i, j]\n",
    "        return X\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    X = np.array([\n",
    "        [5, 3, 0, 1],\n",
    "        [4, 0, 0, 1],\n",
    "        [1, 1, 0, 5],\n",
    "        [1, 0, 0, 4],\n",
    "        [0, 1, 5, 4],\n",
    "    ], dtype=np.float)\n",
    "    # replace 0 with np.nan\n",
    "    X[X == 0] = np.nan\n",
    "    print(X)\n",
    "    # np.random.seed(1)\n",
    "    mf = MF(X, k=2, alpha=0.1, beta=0.1, iterations=100)\n",
    "    mf.train()\n",
    "    X_hat = mf.full_matrix()\n",
    "    X_comp = mf.replace_nan(X_hat)\n",
    " \n",
    "    print(X_hat)\n",
    "    print(X_comp)\n",
    "    print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f172f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
